{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Practice Notebook: Credit Scoring Models for Banking",
      "provenance": [],
      "collapsed_sections": [
        "Yy0Kwwg43_HN",
        "s5W5F3084Ccc",
        "qfNEnBw637RZ",
        "AW8i6nAy4PM6",
        "bN9Hn-5UcN4b",
        "rkdaD8dG229Q",
        "fBrgP4Su5mMt",
        "s7bsk53Y508R",
        "FjnfSF1O508T",
        "T-J4QXei6aZh",
        "yrcsM3vI508Z"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kertich/Abstract-data-types-with-doctstrings/blob/master/Copy_of_Practice_Notebook_Credit_Scoring_Models_for_Banking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVJGoD-k8QbZ"
      },
      "source": [
        "<font color=\"blue\">To use this notebook on Google Colaboratory, you will need to make a copy of it. Go to **File** > **Save a Copy in Drive**. You can then use the new copy that will appear in the new tab.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKZxLFJgsqWV"
      },
      "source": [
        "You will be required to work on this notebook together with your break out room peers. Ensure one person shares their screen while others discuss and provide tips on how to go about the given problem. The facilitator will come into your rooms to provide you the support that you might need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47QIECbq4orp"
      },
      "source": [
        "# Practice Notebook: Credit Scoring Models for Banking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz7EQjs35RQC"
      },
      "source": [
        "## Logistic Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkBOG9_h6PKS"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_nGg6z0Ie7L"
      },
      "source": [
        "When a bank receives a loan application, based on the applicantâ€™s profile the bank has to make a decision regarding whether to go ahead with the loan approval or not. Create a logistic regression model given a dataset with the following variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kAoEzYDHh3n"
      },
      "source": [
        "\n",
        "Below is the glossary for the given dataset:\n",
        "*   Age (numeric) \n",
        "*   Sex (text: male, female)\n",
        "*   Job (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)\n",
        "*   Housing (text: own, rent, or free)\n",
        "*   Saving accounts (text - little, moderate, quite rich, rich)\n",
        "*   Checking account (numeric, in DM - Deutsch Mark)\n",
        "*   Credit amount (numeric, in DM)\n",
        "*   Duration (numeric, in month)\n",
        "*   Purpose (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)\n",
        "*   Risk (Value target - Good or Bad Risk)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy0Kwwg43_HN"
      },
      "source": [
        "#### Importing Required Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRdbFL9sqcWM"
      },
      "source": [
        "We will first import the python libraries that we will need for performing our analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7RU2KnoHO3r"
      },
      "source": [
        "# Importing standard libraries\n",
        "import pandas as pd               # library for data manipulation and analysis\n",
        "import numpy as np                # library for performin scientific computations\n",
        "import matplotlib.pyplot as plt   # library for creating basic visualisations\n",
        "import seaborn as sns             # library for creating rich data visualisations\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5W5F3084Ccc"
      },
      "source": [
        "#### Loading Our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ2IK6gB6PKV"
      },
      "source": [
        "# Importing Dataset for the given URL\n",
        "# ---\n",
        "# Dataset URL = https://bit.ly/35AyjFR\n",
        "# ---\n",
        "#\n",
        "\n",
        "# reading our dataset from a URL and storing contents in a dataframe (tabular datastructure)\n",
        "df = pd.read_csv('https://bit.ly/35AyjFR')\n",
        "\n",
        "# previewing the dataframe\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfNEnBw637RZ"
      },
      "source": [
        "#### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKx5Wmo99fYy"
      },
      "source": [
        "# determining the shape of the dataset / by shape we mean size\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG5a-q944dRI"
      },
      "source": [
        "**Observation:** Our dataset contains 1000 records, 10 variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peRI1Mzy4V44"
      },
      "source": [
        "# generating a statistical summary of numerical variables\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7AhAiFk4jM1"
      },
      "source": [
        "**Observation:** We can see that we not have any missing values since all variables have the same no. of count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPFv02pk585O"
      },
      "source": [
        "# checking the data types: Object = string, int64 = integers\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Dlk4TUPBlL"
      },
      "source": [
        "# dropping 'Unnamed: 0' variable: \n",
        "df = df.drop(columns = ['Unnamed: 0'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE9CmUaIPDd6"
      },
      "source": [
        "We drop this variable because it has no meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiihvKotNgak"
      },
      "source": [
        "# getting the unique values of each variable\n",
        "cols = df.columns.to_list()\n",
        "\n",
        "for col in cols:\n",
        "    print(\"Variable:\", col)\n",
        "    print(\"Number of unique values:\", df[col].nunique())\n",
        "    print(df[col].unique())\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjiFDjoqPSxS"
      },
      "source": [
        "**Observation:** This gives us a sense of which unique values exists within our variables. We can see that the savings account variable has a nan unique value. This is a hint that we have missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXNroKd1SGts"
      },
      "source": [
        "# check for missing values\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVk1gtivSQcq"
      },
      "source": [
        "**Observation:** It seems that we have lots of missing values in the Savings accounts and Checking account variables. From the dataset, we can infer that the missing values are as a result of having persons who took credit from the bank but didn't have a savings accounts or checking account. Thus, we will save these missing values as \"NoSavingAccount\" and \"NoCheckingAccount\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S6PqdxMfPCd"
      },
      "source": [
        "# dealing with missing values\n",
        "df[\"Saving accounts\"].fillna('NoSavingAcc', inplace= True)\n",
        "df[\"Checking account\"].fillna('NoCheckAcc', inplace= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziUNDtRwfaL7"
      },
      "source": [
        "# check for missing values\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3dQrxlmferG"
      },
      "source": [
        "**Observation:** We have no missing values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQu8odLFNwBg"
      },
      "source": [
        "# checking for outliers\n",
        "plt.figure(figsize = (14, 8))\n",
        "df.boxplot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2izvGHSUyah"
      },
      "source": [
        "**Observation:** We see that the credit amount variable has outliers. This would be problematic when using a classification technique like logistic regression which assume that there are no outliers in the data. However, we will not remove these outliers as this data can be genuine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc7BV_k3gFQN"
      },
      "source": [
        "# standardizing variable names for readability\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOeGMQ_0untP"
      },
      "source": [
        " We will no perform univariate analysis. This is a type of analysis done only one variable. This type of analysis will be helpful in understanding the characteristics of each variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv2P35z2Ogpw"
      },
      "source": [
        "# univariate analysis: age variable\n",
        "sns.distplot(df.age);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywDDy4dQh2dK"
      },
      "source": [
        "**Observation:** The 'age' variable is not normally distributed. It is skewed to the right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4XDT1d2gduk"
      },
      "source": [
        "# univariate analysis: sex variable\n",
        "plt.figure(figsize = (5, 4))\n",
        "df.sex.value_counts().plot(kind = 'bar', rot = 0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm-JKvICu1Ic"
      },
      "source": [
        "**Observation:** There are more male records in our dataset than female records. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0BpXA1piylU"
      },
      "source": [
        "# univariate analysis: job variable\n",
        "plt.figure(figsize = (5, 4))\n",
        "df.job.value_counts().plot(kind = 'bar', rot = 0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCm6koxvriLv"
      },
      "source": [
        "**Your observation?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPeEPofvjre9"
      },
      "source": [
        "# univariate analysis: housing variable\n",
        "plt.figure(figsize = (5, 4))\n",
        "df.housing.value_counts().plot(kind = 'bar', rot = 0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyEjTpA-rlYd"
      },
      "source": [
        "**Your observation?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAu1zXbbjvnm"
      },
      "source": [
        "# univariate analysis: checking_account variable\n",
        "plt.figure(figsize = (8, 4))\n",
        "df.saving_accounts.value_counts().plot(kind = 'bar', rot = 0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSC54tUVrmnj"
      },
      "source": [
        "**Your observation?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtbwVqJVjzoT"
      },
      "source": [
        "# univariate analysis: checking_account variable\n",
        "plt.figure(figsize = (5, 4))\n",
        "df.checking_account.value_counts().plot(kind = 'bar', rot = 0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9jZrzuhrn1w"
      },
      "source": [
        "**Your observation?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHsLL97Wj-jC"
      },
      "source": [
        "# univariate analysis: credit_amount variable\n",
        "sns.distplot(df.credit_amount);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCVs3GnZroy9"
      },
      "source": [
        "**Your observation?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fEhmb2HkCgl"
      },
      "source": [
        "# univariate analysis: purpose variable\n",
        "plt.figure(figsize = (15, 6))\n",
        "df.purpose.value_counts().plot(kind = 'bar', rot = 0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrcbkPFDrp-C"
      },
      "source": [
        "**Your observation?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja2MYz2HkGDG"
      },
      "source": [
        "# univariate analysis: duration variable\n",
        "sns.distplot(df.duration); "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pIvBm7ykLnO"
      },
      "source": [
        "# univariate analysis: purpose variable\n",
        "plt.figure(figsize = (5, 4))\n",
        "df.risk.value_counts().plot(kind = 'bar', rot = 0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdj-rP3RvF38"
      },
      "source": [
        "We now perform bivariate analysis, which is a type of analysis that involves two variables. The main objective is to understand the relationships between these two types of variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GQWv61lUybk"
      },
      "source": [
        "We will look at the relationship between our target variable 'risk' and the other variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BubQ9fl4nuD_"
      },
      "source": [
        "# bivariate analysis: risk by sex variables\n",
        "plt.figure(figsize = (6, 5))\n",
        "sns.countplot('sex', hue = 'risk', data = df);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HaqKlIkmw9E"
      },
      "source": [
        "# risk by job variables\n",
        "plt.figure(figsize = (6, 5))\n",
        "sns.countplot('job', hue = 'risk', data = df);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9qKruNndm5"
      },
      "source": [
        "# risk by housing variables\n",
        "plt.figure(figsize = (6, 5))\n",
        "sns.countplot('housing', hue = 'risk', data = df);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPJyZHqonlJK"
      },
      "source": [
        "# risk by checking_account variables\n",
        "plt.figure(figsize = (6, 5))\n",
        "sns.countplot('checking_account', hue = 'risk', data = df);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG8pmjKDoZ69"
      },
      "source": [
        "# risk by purpose variables\n",
        "plt.figure(figsize = (15, 5))\n",
        "sns.countplot('purpose', hue = 'risk', data = df);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KLwRq0fqgtk"
      },
      "source": [
        "# heatmap to visualise features with linear relationships \n",
        "plt.figure(figsize = (8, 8))\n",
        "sns.heatmap(df.corr(), annot=True, );"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jpgWkx_rn6S"
      },
      "source": [
        "**Observation:** The variables duration and credit amount are strongly correlated. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW8i6nAy4PM6"
      },
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WUfpprG65S4"
      },
      "source": [
        "In order to work with our target variable, which is the risk variable, we will binarize our risk variable. Labels are as follows: 0: Bad 1: Good. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8-3y1QC6_PL"
      },
      "source": [
        "# LabelBinarizer converts the string categorical variable to binary \n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb= LabelBinarizer()\n",
        "df[\"risk\"]= lb.fit_transform(df[\"risk\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXrSCsse62e6"
      },
      "source": [
        "# plotting risk distribution to understand whether there are more records \n",
        "# with more categories than the other.\n",
        "sns.countplot('risk', data = df);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "793cdUTc8XD0"
      },
      "source": [
        "**Observation:** We note that our dataset is highly inbalanced. We need to make sure that our dataset is balanced which means, we need to have the same no. of records with the 0 and 1 class. We will address this issue later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heOmjB4W8rPV"
      },
      "source": [
        "For now, in order to prepare our dataset for our model, we will perform binning on the numerical variables i.e. duration, age and credit amount variables. Once we perform binning, we will later perform one hot encoding to all the categorical variables within our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW3AYFbt8pwQ"
      },
      "source": [
        "# performing binning on duration \n",
        "# we bin by specifying 6 bins\n",
        "df[\"duration\"] = pd.qcut(df.duration, q = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2DcomIR80y_"
      },
      "source": [
        "# performing binning on age\n",
        "# we bin by defining the intervals and categories\n",
        "interval = (18, 25, 35, 60, 120)\n",
        "categories = ['student', 'youth', 'adult', 'senior']\n",
        "df[\"age\"] = pd.cut(df.age, interval, labels = categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmkCKK9uJFxl"
      },
      "source": [
        "# performing binning on credit_amount\n",
        "# this time we bin without specifying i.e. we create 6 bins\n",
        "df[\"credit_amount\"] = pd.qcut(df.credit_amount, q = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlsQQnU6tBPS"
      },
      "source": [
        "We will now transform our features into dummy variables by one-hot encoding the features in our dataset. This will make our features robust for our linear regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZPQykjhKUyi"
      },
      "source": [
        "# performing one hot encoding to the purpose variable\n",
        "df = df.merge(pd.get_dummies(df.job, drop_first=True, prefix='job'), left_index=True, right_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQt5EO2P5k7s"
      },
      "source": [
        "# performing one hot encoding to the purpose variable\n",
        "df = df.merge(pd.get_dummies(df.purpose, drop_first=True, prefix='purpose'), left_index=True, right_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOBxpxT86C6P"
      },
      "source": [
        "# performing one hot encoding to the sex variable\n",
        "df = df.merge(pd.get_dummies(df.sex, drop_first=True, prefix='sex'), left_index=True, right_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYk0o2E76E67"
      },
      "source": [
        "# performing one hot encoding to the housing variable\n",
        "df = df.merge(pd.get_dummies(df.housing, drop_first=True, prefix='housing'), left_index=True, right_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-WxDq1o6HOG"
      },
      "source": [
        "# performing one hot encoding to the savings account variable\n",
        "df = df.merge(pd.get_dummies(df.saving_accounts, drop_first=True, prefix='savings'), left_index=True, right_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uB8sOjh6KOz"
      },
      "source": [
        "# performing one hot encoding to checking account variable\n",
        "df = df.merge(pd.get_dummies(df.checking_account, drop_first=True, prefix='check'), left_index=True, right_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41xOwEPi6MPc"
      },
      "source": [
        "# performing one hot encoding to age variable\n",
        "df = df.merge(pd.get_dummies(df.age, drop_first=True, prefix='age'), left_index=True, right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY_as3R__2dL"
      },
      "source": [
        "# performing one hot encoding to age variable\n",
        "df = df.merge(pd.get_dummies(df.duration, drop_first=True, prefix='duration'), left_index=True, right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyH3noHyJmR5"
      },
      "source": [
        "# performing one hot encoding to credit amount variable\n",
        "df = df.merge(pd.get_dummies(df.credit_amount, drop_first=True, prefix='credit_amount'), left_index=True, right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s51JILRI_hNd"
      },
      "source": [
        "# previewing our created dataframe\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vieLd1J_-h0v"
      },
      "source": [
        "# we then cxcluding the missing columns since we have new variables for use\n",
        "del df[\"job\"]\n",
        "del df[\"purpose\"]\n",
        "del df[\"sex\"]\n",
        "del df[\"housing\"]\n",
        "del df[\"saving_accounts\"]\n",
        "del df[\"checking_account\"]\n",
        "del df[\"age\"]\n",
        "del df[\"duration\"]\n",
        "del df[\"credit_amount\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhA_6bhQtlec"
      },
      "source": [
        "# preview updated dataframe\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaBPmPzX9ibH"
      },
      "source": [
        "# dividing our dataset into features (X) and target (y)\n",
        "X = df.drop(columns = ['risk']).values\n",
        "y = df['risk'].values\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsx_kFFkSUO5"
      },
      "source": [
        "# splitting our dataset into 80-20 train-test sets\n",
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMX2zyf9Z4XQ"
      },
      "source": [
        "Because we had earlier seen that we had an imbalanced dataset, we will create a balanced dataset by trying to resample our dataset using SMOTE (Synthetic minority Oversampling Technique). This technique works randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfj-DCF8Z2rm"
      },
      "source": [
        "# creating a balanced dataset\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smt = SMOTE()\n",
        "X_train, y_train = smt.fit_sample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caldshkvcEY0"
      },
      "source": [
        "# we check the amount of records in each category\n",
        "np.bincount(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRcdxyxPk3WZ"
      },
      "source": [
        "**Observation:** The minority class has been increased to the total number of majority class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN9Hn-5UcN4b"
      },
      "source": [
        "#### Data Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhmZ-8Mn-EEu"
      },
      "source": [
        "# model creation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_classifier = LogisticRegression()\n",
        "\n",
        "# training our model\n",
        "logistic_classifier.fit(X_train, y_train)\n",
        "\n",
        "# making predictions\n",
        "y_pred_logistic = logistic_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkdaD8dG229Q"
      },
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyZsk-38-MMF"
      },
      "source": [
        "# model evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "print(accuracy_score(y_pred_logistic, y_test))\n",
        "print(confusion_matrix(y_test, y_pred_logistic))\n",
        "print(classification_report(y_test, y_pred_logistic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQBZMgCqk_C6"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "Our accuracy score for this model was 0.73. \n",
        "\n",
        "From our confusion matrix, we can see that we 119 records with the class 1 were predicted correctly, however 32 records with the class 1 were incorrectly predicted. 28 records with the class 0 were predicted correctly while 21 records were predicted incorrectly.\n",
        "\n",
        "From our classification report, we can see that we have a recall of 0.66 which is our true positive rate i.e. When it's actually a 1, how often does it predict a 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtiJj_4WWsXB"
      },
      "source": [
        "# Exploring another metric below \n",
        "# ---\n",
        "# plotting roc curve (receiving operating characteristic curve)\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Create true and false positive rates\n",
        "false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_pred_logistic)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(false_positive_rate, true_positive_rate)\n",
        "plt.plot([0, 1], ls=\"--\")\n",
        "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjxLylMgXteY"
      },
      "source": [
        "This type of curve shows the true positive and false positive rate for every probability threshold of a binary classifier. The higher the blue line, the better the model at distinguishing between the positive and negative classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyPMDZS0nIMI"
      },
      "source": [
        "# roc_auc_score\n",
        "roc_auc_score(y_test, y_pred_logistic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNaaoaBonO8Q"
      },
      "source": [
        "Observation: An roc auc score coser to 1 the better the model is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBrgP4Su5mMt"
      },
      "source": [
        "### <font color=\"green\">Challenge 1</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnQ9C61T5mMu"
      },
      "source": [
        "# Create a credit scoring model that will be used to predict \n",
        "# whether a customer will default or not.\n",
        "# ---\n",
        "# Dataset URL = https://bit.ly/37FPsAF\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7bsk53Y508R"
      },
      "source": [
        "## Advanced Machine Learning Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13CyddVm2Q0m"
      },
      "source": [
        "For the following examples, we will apply other advanced machine learning techniques i.e. decision trees, random forest, gradient boosting and neural networks. We will dive right into the modeling stage since we will be using the dataset that we have already explored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjnfSF1O508T"
      },
      "source": [
        "### Example: Decision Trees, Random Forest and Gradient Boosting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06NRjBav508U"
      },
      "source": [
        "# importing our machine learning algorithms  \n",
        "from sklearn.tree import DecisionTreeClassifier    \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# instantiating our algorithms\n",
        "decision_classifier = DecisionTreeClassifier()\n",
        "random_forest_classifier = RandomForestClassifier()\n",
        "gbm_classifier = GradientBoostingClassifier()\n",
        "\n",
        "# training our models\n",
        "decision_classifier.fit(X_train, y_train)\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "gbm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# making predictions\n",
        "decision_y_prediction = decision_classifier.predict(X_test) \n",
        "random_forest_y_pred = random_forest_classifier.predict(X_test)\n",
        "gbm_y_pred = gbm_classifier.predict(X_test)\n",
        "\n",
        "# evaluation metrics\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
        "print('Decision Tree')\n",
        "print(accuracy_score(decision_y_prediction, y_test))\n",
        "print(confusion_matrix(decision_y_prediction, y_test))\n",
        "print(classification_report(decision_y_prediction, y_test))\n",
        "print(roc_auc_score(decision_y_prediction, y_test))\n",
        "\n",
        "print('Random Forest')\n",
        "print(accuracy_score(random_forest_y_pred, y_test))\n",
        "print(confusion_matrix(random_forest_y_pred, y_test))\n",
        "print(classification_report(random_forest_y_pred, y_test))\n",
        "print(roc_auc_score(random_forest_y_pred, y_test))\n",
        "\n",
        "print('Gradient Boosting')\n",
        "print(accuracy_score(gbm_y_pred, y_test))\n",
        "print(confusion_matrix(gbm_y_pred, y_test))\n",
        "print(classification_report(gbm_y_pred, y_test))\n",
        "print(roc_auc_score(gbm_y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxOwJKgF5qSb"
      },
      "source": [
        "**Observation:** The Recall score for the random forest model was slightly higher than that of the logistic regression model. However, the random forest model was not better at predicting true negatives. \n",
        "\n",
        "Overall, we can still retain to use the logistic regression classifier, as it still outperformed other models even without any optimisation done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-J4QXei6aZh"
      },
      "source": [
        "### Example: Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNdRdQRW6hGU"
      },
      "source": [
        "# importing libraries\n",
        "import keras\n",
        "from keras.models import Sequential     # Used to initialize the Artificial Neural Network\n",
        "from keras.layers import Dense          # Used to build the hidden Layers\n",
        "from keras.layers import Dropout        # Used to prevent overfitting\n",
        "\n",
        "# creating an instance \n",
        "classifier = Sequential() \n",
        "\n",
        "# adding hidden layers\n",
        "classifier.add(Dense(units = 100, input_dim = 33, activation = 'relu'))\n",
        "classifier.add(Dropout(0.3, seed = 2))\n",
        "classifier.add(Dense(units = 100, activation = 'relu'))\n",
        "classifier.add(Dropout(0.3, seed = 2))\n",
        "\n",
        "# output layer\n",
        "classifier.add(Dense(units = 1, activation = \"sigmoid\"))\n",
        "\n",
        "# compiling our model\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# training our model\n",
        "classifier.fit(X_train, y_train, epochs = 500, batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugv6ynKg7KDe"
      },
      "source": [
        "# model evaluation\n",
        "loss, accuracy = classifier.evaluate(X_test, y_test)\n",
        "print('ANN Accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0V6-Bon7pxO"
      },
      "source": [
        "We get an accuracy of 0.69 with our neural networks model. However, this is not better than our logistic regression accuracy score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrcsM3vI508Z"
      },
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VWi_X6v508a"
      },
      "source": [
        "# Given the dataset in challenge 1, create a credit scoring model\n",
        "# to predict whether a customer will default or not using any of the\n",
        "# advanced machine learning techniques.\n",
        "# ---\n",
        "# Dataset URL = https://bit.ly/37FPsAF\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}